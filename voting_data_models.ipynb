{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# <center>Group Project 1: Supervised Learning</center>\n",
    "## <center>Josh Melton and Ivan Benitez</center>  \n",
    "\n",
    "### Part 2: Congressional Voting Data\n",
    "#### a) Data Preparation\n",
    "\n",
    "First, read the voting data csv into a pandas dataframe.  \n",
    "- The data contain yes/no voting records on various issues: 'y' is converted to 1 and 'n' to 0.\n",
    "- The class label column contains each congressperson's party affiliation. For simplicity, republican is converted to 1 and democrat to 0.\n",
    "- The data also contain missing values, encoded with a '?'. These values are converted to numpy NaN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "voting_data = pd.read_csv('voting_data.csv',\n",
    "                          header=0,\n",
    "                          index_col=False)\n",
    "\n",
    "voting_data.replace('?', np.NaN, inplace=True)\n",
    "voting_data.replace('y', 1, inplace=True)\n",
    "voting_data.replace('n', 0, inplace=True)\n",
    "\n",
    "voting_data.replace('republican', 1, inplace=True)\n",
    "voting_data.replace('democrat', 0, inplace=True)\n",
    "\n",
    "# voting_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three different methods for handling the missing data were used to create three versions of the dataset\n",
    "- Version 1: Remove rows with missing values\n",
    "- Version 2: Replace missing values with a third label (2)\n",
    "- Version 3: Replace missing values with the mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Version 1: Drop rows with NaN\n",
    "voting_data_v1 = voting_data.dropna(axis=0, how='any')\n",
    "labels_v1 = voting_data_v1['Class Name']\n",
    "features_v1 = voting_data_v1.drop('Class Name', axis=1)\n",
    "\n",
    "# Version 2: Replace NaN with third category (2)\n",
    "voting_data_v2 = voting_data.fillna(2)\n",
    "labels_v2 = voting_data_v2['Class Name']\n",
    "features_v2 = voting_data_v2.drop('Class Name', axis=1)\n",
    "\n",
    "# Version 3: Replace NaN with mode (most frequent)\n",
    "labels_v3 = voting_data['Class Name']\n",
    "features_v3 = voting_data.drop('Class Name', axis=1)\n",
    "\n",
    "imp = SimpleImputer(strategy='most_frequent')\n",
    "features_v3 = imp.fit_transform(features_v3)\n",
    "\n",
    "features = [features_v1, features_v2, features_v3]\n",
    "labels = [labels_v1, labels_v2, labels_v3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Functions to print formatted metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def print_metrics(labels, preds):\n",
    "    \"\"\"\n",
    "        Prints confusion matrix and metrics scores for a binary classification\n",
    "    \"\"\"\n",
    "    scores = metrics.precision_recall_fscore_support(labels, preds)\n",
    "    conf = metrics.confusion_matrix(labels, preds)\n",
    "    print(' ' * 4 + 'Confusion Matrix')\n",
    "    print(' ' * 17 + 'Predict Positive    Predict Negative')\n",
    "    print('Actual Positive         {}                 {}'.format(conf[1, 1], conf[1, 0]))\n",
    "    print('Actual Negative         {}                 {}'.format(conf[0, 1], conf[0, 0]))\n",
    "    print()\n",
    "    print('Accuracy: {:.3f}'.format(metrics.accuracy_score(labels, preds)))\n",
    "    print()\n",
    "    print(' ' * 4 + 'Classification Report')\n",
    "    print(' ' * 11 + 'Positive    Negative')\n",
    "    print('Num cases    {}           {}'.format(scores[3][1], scores[3][0]))\n",
    "    print('Precision    {:.2f}       {:.2f}'.format(scores[0][1], scores[0][0]))\n",
    "    print('Recall       {:.2f}       {:.2f}'.format(scores[1][1], scores[1][0]))\n",
    "    print('F1 Score     {:.2f}       {:.2f}'.format(scores[2][1], scores[2][0]))\n",
    "\n",
    "def print_cv_scores(results):\n",
    "    \"\"\"\n",
    "        Prints scoring metrics from cross-validation \n",
    "    \"\"\"\n",
    "    f1 = results['test_f1']\n",
    "    precision = results['test_precision']\n",
    "    recall = results['test_recall']\n",
    "    accuracy = results['test_accuracy']\n",
    "\n",
    "    print(' ' * 4 + 'Cross Validation Scores')\n",
    "    print(' ' * 9 + 'F1     Precision    Recall    Accuracy')\n",
    "    for i, (f, p, r, a) in enumerate(zip(f1, precision, recall, accuracy)):\n",
    "        print('Fold {}   {:.3f}    {:.3f}      {:.3f}     {:.3f}'.format(i+1, f, p, r, a))\n",
    "    print()\n",
    "    print('Mean F1: {:.3f}'.format(f1.mean()))\n",
    "    print('Mean Precision: {:.3f}'.format(precision.mean()))\n",
    "    print('Mean Recall: {:.3f}'.format(recall.mean()))\n",
    "    print('Mean Accuracy: {:.3f}'.format(accuracy.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each version of the data, initialize and fit a Decision Tree classifier and a Naive Bayes model.  \n",
    "Evaluate the models on the test data and then run 5-fold cross validation on the whole data set.  \n",
    "F1, precision, recall, and accuracy scores are reported for each fold as well as the mean score across all folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Version 1: Decision Tree\n    Confusion Matrix\n                 Predict Positive    Predict Negative\nActual Positive         19                 3\nActual Negative         1                 24\n\nAccuracy: 0.915\n\n    Classification Report\n           Positive    Negative\nNum cases    22           25\nPrecision    0.95       0.89\nRecall       0.86       0.96\nF1 Score     0.90       0.92\n\n",
      "    Cross Validation Scores\n         F1     Precision    Recall    Accuracy\nFold 1   0.933    0.913      0.955     0.936\nFold 2   0.917    0.846      1.000     0.915\nFold 3   0.977    1.000      0.955     0.979\nFold 4   0.950    1.000      0.905     0.957\nFold 5   0.884    0.864      0.905     0.889\n\nMean F1: 0.932\nMean Precision: 0.925\nMean Recall: 0.944\nMean Accuracy: 0.935\n-------------------------\nVersion 1: Naive Bayes\n    Confusion Matrix\n                 Predict Positive    Predict Negative\nActual Positive         20                 2\nActual Negative         7                 18\n\nAccuracy: 0.809\n\n    Classification Report\n           Positive    Negative\nNum cases    22           25\nPrecision    0.74       0.90\nRecall       0.91       0.72\nF1 Score     0.82       0.80\n\n    Cross Validation Scores\n         F1     Precision    Recall    Accuracy\nFold 1   0.894    0.840      0.955     0.894\nFold 2   0.826    0.792      0.864     0.830\nFold 3   0.977    1.000      0.955     0.979\nFold 4   1.000    1.000      1.000     1.000\nFold 5   0.870    0.800      0.952     0.867\n\nMean F1: 0.913\nMean Precision: 0.886\nMean Recall: 0.945\nMean Accuracy: 0.914\n##################################################\nVersion 2: Decision Tree\n    Confusion Matrix\n                 Predict Positive    Predict Negative\nActual Positive         26                 1\nActual Negative         4                 56\n\nAccuracy: 0.943\n\n    Classification Report\n           Positive    Negative\nNum cases    27           60\nPrecision    0.87       0.98\nRecall       0.96       0.93\nF1 Score     0.91       0.96\n\n    Cross Validation Scores\n         F1     Precision    Recall    Accuracy\nFold 1   0.971    0.971      0.971     0.977\nFold 2   0.971    0.971      0.971     0.977\nFold 3   0.955    0.970      0.941     0.966\nFold 4   0.889    0.933      0.848     0.919\nFold 5   0.923    0.938      0.909     0.942\n\nMean F1: 0.942\nMean Precision: 0.956\nMean Recall: 0.928\nMean Accuracy: 0.956\n-------------------------\nVersion 2: Naive Bayes\n    Confusion Matrix\n                 Predict Positive    Predict Negative\nActual Positive         25                 2\nActual Negative         8                 52\n\nAccuracy: 0.885\n\n    Classification Report\n           Positive    Negative\nNum cases    27           60\nPrecision    0.76       0.96\nRecall       0.93       0.87\nF1 Score     0.83       0.91\n\n    Cross Validation Scores\n         F1     Precision    Recall    Accuracy\nFold 1   0.889    0.842      0.941     0.909\nFold 2   0.822    0.769      0.882     0.852\nFold 3   0.923    0.968      0.882     0.943\nFold 4   0.955    0.941      0.970     0.965\nFold 5   0.800    0.714      0.909     0.826\n\nMean F1: 0.878\nMean Precision: 0.847\nMean Recall: 0.917\nMean Accuracy: 0.899\n##################################################\n",
      "Version 3: Decision Tree\n    Confusion Matrix\n                 Predict Positive    Predict Negative\nActual Positive         24                 3\nActual Negative         5                 55\n\nAccuracy: 0.908\n\n    Classification Report\n           Positive    Negative\nNum cases    27           60\nPrecision    0.83       0.95\nRecall       0.89       0.92\nF1 Score     0.86       0.93\n\n    Cross Validation Scores\n         F1     Precision    Recall    Accuracy\nFold 1   0.971    0.971      0.971     0.977\nFold 2   0.932    0.872      1.000     0.943\nFold 3   0.955    0.970      0.941     0.966\nFold 4   0.892    0.906      0.879     0.919\nFold 5   0.928    0.889      0.970     0.942\n\nMean F1: 0.935\nMean Precision: 0.921\nMean Recall: 0.952\nMean Accuracy: 0.949\n-------------------------\nVersion 3: Naive Bayes\n    Confusion Matrix",
      "\n                 Predict Positive    Predict Negative\nActual Positive         24                 3\nActual Negative         7                 53\n\nAccuracy: 0.885\n\n    Classification Report\n           Positive    Negative\nNum cases    27           60\nPrecision    0.77       0.95\nRecall       0.89       0.88\nF1 Score     0.83       0.91\n\n    Cross Validation Scores\n         F1     Precision    Recall    Accuracy\nFold 1   0.889    0.842      0.941     0.909\nFold 2   0.838    0.775      0.912     0.864\nFold 3   0.923    0.968      0.882     0.943\nFold 4   0.955    0.941      0.970     0.965\nFold 5   0.811    0.732      0.909     0.837\n\nMean F1: 0.883\nMean Precision: 0.852\nMean Recall: 0.923\nMean Accuracy: 0.904\n##################################################\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import sklearn.model_selection as ms\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "scoring = ['f1', 'precision', 'recall', 'accuracy']\n",
    "for i, (feat, lab) in enumerate(zip(features, labels)):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = ms.train_test_split(feat, lab,\n",
    "                                                           test_size=0.2,\n",
    "                                                           random_state=123)\n",
    "\n",
    "    # Decision Tree model\n",
    "    print('Version {}: Decision Tree'.format(i+1))\n",
    "    tree = DecisionTreeClassifier(criterion='gini',\n",
    "                                  class_weight='balanced')\n",
    "    tree.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_tree = tree.predict(X_test)\n",
    "    print_metrics(y_test, y_pred_tree)\n",
    "    print()\n",
    "\n",
    "    tree_cv_scores = ms.cross_validate(tree, feat, lab,\n",
    "                                       cv=5, scoring=scoring)\n",
    "    print_cv_scores(tree_cv_scores)\n",
    "    print('-' * 25)\n",
    "\n",
    "    # Naive Bayes model\n",
    "    print('Version {}: Naive Bayes'.format(i+1))\n",
    "    bnb = BernoulliNB()\n",
    "    bnb.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_nb = bnb.predict(X_test)\n",
    "    print_metrics(y_test, y_pred_nb)\n",
    "    print()\n",
    "\n",
    "    bnb_cv_scores = ms.cross_validate(bnb, feat, lab,\n",
    "                                      cv=5, scoring=scoring)\n",
    "    print_cv_scores(bnb_cv_scores)\n",
    "    print('#' * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}